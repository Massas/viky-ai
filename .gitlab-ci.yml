stages:
  - prerequist
  - build
  - test
  - docker_tag
  - deploy
  - admin

image: docker:stable

.kubernetes_runner: &kubernetes_runner
  tags:
    - kubernetes

variables:
  SKIP_TEST: "true"
  CI_SERVICE_HOST: "127.0.0.1"
  GIT_SUBMODULE_STRATEGY: recursive
  DOCKER_HOST: "tcp://dood:2375"
  DOCKER_DRIVER: overlay2
  DOCKER_IMAGE_APP: $CI_REGISTRY_IMAGE/app
  DOCKER_IMAGE_NLP: $CI_REGISTRY_IMAGE/nlp
  DOCKER_IMAGE_TOOLS_RUBY: $CI_REGISTRY_IMAGE/tools_ruby_base
  DOCKER_IMAGE_TOOLS_BACKUP: $CI_REGISTRY_IMAGE/tools_backup
  # Needed for Gitlab Managed
  TILLER_NAMESPACE: ${KUBE_NAMESPACE}
  HELM_HOST: "localhost:44134"

# Build docker image

build_ruby_base:
  <<: *kubernetes_runner
  stage: prerequist
  script:
    - docker_build ${DOCKER_IMAGE_TOOLS_RUBY}      deployment/docker_tools/ruby_base "--target image_ruby_base"
    - docker_build ${DOCKER_IMAGE_TOOLS_RUBY}_test deployment/docker_tools/ruby_base "--target image_ruby_base_test"

build_backup:
  <<: *kubernetes_runner
  stage: prerequist
  script:
    - docker_build ${DOCKER_IMAGE_TOOLS_BACKUP} deployment/docker_tools/backup "--build-arg VIKYAPP_BACKUP_PASSWORD=${VIKYAPP_BACKUP_PASSWORD}"

elastic_keystore:
  <<: *kubernetes_runner
  stage: prerequist
  variables:
    ES_IGNORE_SECRET_GENERATION: "true"
  script:
    - chmod +x ./deployment/kubernetes/bin/generateS3Keystore.sh
    - deployment/kubernetes/bin/generateS3Keystore.sh
    - ls -l
  artifacts:
    paths:
      - elasticsearch.keystore
    # expire_in: 1 hour

build_webapp:
  <<: *kubernetes_runner
  stage: build
  script:
    - docker_build ${DOCKER_IMAGE_APP}      webapp "--build-arg VIKY_IMAGE_TAG=${CI_COMMIT_REF_SLUG} --target image_ruby_run"

build_webapp_test:
  <<: *kubernetes_runner
  stage: build
  script:
    - docker_build ${DOCKER_IMAGE_APP}_test webapp "--build-arg VIKY_IMAGE_TAG=${CI_COMMIT_REF_SLUG} --target image_ruby_test"

build_nlp:
  <<: *kubernetes_runner
  stage: build
  script:
    - docker_build ${DOCKER_IMAGE_NLP}      nlp "--target run_image"
    - docker_build ${DOCKER_IMAGE_NLP}_test nlp "--target test_image"

# Test
.test_webapp: &test_webapp
  stage: test
  image: ${DOCKER_IMAGE_APP}_test:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}
  variables:
    POSTGRES_USER: superman
    POSTGRES_PASSWORD: sup_3rman
    POSTGRES_DB: vikyapp_test
  services:
    - postgres:9.6-alpine
    - redis:3.2-alpine
    - name: docker.elastic.co/elasticsearch/elasticsearch:6.6.1
      command: ["bin/elasticsearch", "-Ediscovery.type=single-node"] # Important other wise, port 9200 will never be exposed
  before_script:
    - export VIKYAPP_DB_USERNAME="${POSTGRES_USER}"
    - export VIKYAPP_DB_PASSWORD="${POSTGRES_PASSWORD}"
    - export VIKYAPP_DB_HOST="${CI_SERVICE_HOST:-postgres}"
    - export VIKYAPP_ACTIONCABLE_REDIS_URL="redis://${CI_SERVICE_HOST:-redis}:6379/1"
    - export VIKYAPP_ACTIVEJOB_REDIS_URL="redis://${CI_SERVICE_HOST:-redis}:6379/2"
    - export VIKYAPP_STATISTICS_URL="http://${CI_SERVICE_HOST:-elasticsearch}:9200"
    - cd /webapp
  artifacts:
    # expire_in: 1 week
    reports:
      junit: "reports/TEST-*.xml"

test_webapp_unit:
  <<: *kubernetes_runner
  <<: *test_webapp
  script:
    - cd /webapp
    - if [ "${SKIP_TEST:-false}" != "true" ]; then ./bin/docker_run_test.sh unit ; fi

# Test
test_webapp_system:
  <<: *kubernetes_runner
  <<: *test_webapp
  script:
    - if [ "${SKIP_TEST:-false}" != "true" ]; then ./bin/docker_run_test.sh system ; fi

test_nlp:
  <<: *kubernetes_runner
  stage: test
  image: ${DOCKER_IMAGE_NLP}_test:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}
  services:
    - redis:3.2-alpine
  variables:
    GIT_STRATEGY: none
  script:
    - export VIKYAPP_REDIS_PACKAGE_NOTIFIER="redis://${CI_SERVICE_HOST:-redis}:6379/3"
    - if [ "${SKIP_TEST:-false}" != "true" ]; then /docker_run_test.sh ; fi
  artifacts:
    # expire_in: 1 week
    reports:
      junit:
        - "reports/TEST-*.xml"

# Tag docker image to lasted
tag_webapp:
  <<: *kubernetes_runner
  stage: docker_tag
  only:
    - develop
  variables:
    GIT_STRATEGY: none
  script:
    - docker_tag_latest ${DOCKER_IMAGE_APP}
    - docker_tag_latest ${DOCKER_IMAGE_APP}_test

tag_nlp:
  <<: *kubernetes_runner
  stage: docker_tag
  only:
    - develop
  variables:
    GIT_STRATEGY: none
  script:
    - docker_tag_latest ${DOCKER_IMAGE_NLP}

tag_ruby_base:
  <<: *kubernetes_runner
  stage: docker_tag
  only:
    - develop
  variables:
    GIT_STRATEGY: none
  script:
    - docker_tag_latest ${DOCKER_IMAGE_TOOLS_RUBY}
    - docker_tag_latest ${DOCKER_IMAGE_TOOLS_RUBY}_test

tag_backup:
  <<: *kubernetes_runner
  stage: docker_tag
  only:
    - develop
  variables:
    GIT_STRATEGY: none
  script:
    - docker_tag_latest ${DOCKER_IMAGE_TOOLS_BACKUP}

# Deploy any branches on rancher dev
deploy_dev:
  <<: *kubernetes_runner
  stage: deploy
  except:
    - master
    - tags
  image: "registry.gitlab.com/gitlab-org/cluster-integration/auto-deploy-image:v0.1.0"
  dependencies:
    - elastic_keystore
    - build_webapp
    - build_nlp
  environment:
    name: ${CI_COMMIT_REF_NAME}
    url: https://viky-${KUBE_NAMESPACE}-kube.viky.ai
    on_stop: delete_env
  variables:
    VIKY_IMAGE_TAG: ${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}
    S3_ACCESS_KEY: ${S3_ACCESS_KEY}
    S3_SECRET_KEY: ${S3_SECRET_KEY}
    VIKY_DOMAIN: viky-${KUBE_NAMESPACE}-kube.viky.ai
  before_script:
    # Init helm
    - helm init --client-only --wait
    - auto-deploy ensure_namespace
    - auto-deploy initialize_tiller
    - auto-deploy create_secret
    - echo "Create ES Keystore secret"
    - kubectl create secret generic es-keystore-secret --namespace "${KUBE_NAMESPACE}" --from-file "elasticsearch.keystore"  || true
    # Go to Kubernetes directory
    - cd deployment/kubernetes/
    - echo "Generate Override values"
    - bin/generateOverrideValuesFile.sh
  script:
    # Deploy Viky Infrastructure
    - cat custom-values.yml
    - helm upgrade --force --wait --values custom-values.yml,viky-infra/custom-values/production.yaml --install "${KUBE_NAMESPACE}-infra" --timeout 600 --namespace "${KUBE_NAMESPACE}" viky-infra/
    # Deploy Viky Platform
    - helm upgrade --force --wait --values custom-values.yml,viky-platform/custom-values/production.yaml --install "${KUBE_NAMESPACE}-platform" --namespace "${KUBE_NAMESPACE}" viky-platform/

# Deploy master on rancher preprod
deploy_preprod:
  <<: *kubernetes_runner
  stage: deploy
  only:
    - master
  except:
    - tags
  image: dtzar/helm-kubectl:2.13.1
  dependencies:
    - elastic_keystore
    - build_webapp
    - build_nlp
  environment:
    name: viky-preprod
    url: https://viky-beta-kube.viky.ai
  variables:
    VIKYAPP_AUTO_BACKUP: "true"
    VIKY_IMAGE_TAG: ${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}
    S3_ACCESS_KEY: ${S3_ACCESS_KEY}
    S3_SECRET_KEY: ${S3_SECRET_KEY}
    NAMESPACE: beta
    VIKY_DOMAIN: viky-beta-kube.viky.ai
  script:
    - export KUBECONFIG=$KUBECONFIG
    - echo "Init helm ..."
    - helm init --client-only --wait
    - echo "Create namespace"
    - kubectl create namespace "${NAMESPACE}" || true
    - echo "Create old secrets"
    - kubectl delete secret es-keystore-secret --namespace "viky-${CI_ENVIRONMENT_SLUG}" || true
    - kubectl delete secret pertimm-registry --namespace "viky-${CI_ENVIRONMENT_SLUG}" || true
    - echo "Create Pertimm Registry Secret"
    - kubectl create secret docker-registry pertimm-registry --namespace "${NAMESPACE}" --docker-server="${PERTIMM_REGISTRY_URL}" --docker-username="${PERTIMM_REGISTRY_USERNAME}" --docker-password="${PERTIMM_REGISTRY_PASSWORD}" --docker-email="${PERTIMM_REGISTRY_EMAIL}" || true
    - echo "Create ES Keystore secret"
    - kubectl create secret generic es-keystore-secret --namespace="${NAMESPACE}" --from-file "elasticsearch.keystore"  || true
    - echo "Generate Override values"
    - deployment/kubernetes/bin/generateOverrideValuesFile.sh
    - echo "Deploy Viky Infrastructure"
    - cat custom-values.yml
    - helm upgrade --force --wait --values custom-values.yml,deployment/kubernetes/viky-infra/custom-values/production.yaml --install "${NAMESPACE}-infra" --timeout 600 --namespace "${NAMESPACE}" deployment/kubernetes/viky-infra/
    - echo "Deploy Viky Platform"
    - helm upgrade --force --wait --values custom-values.yml,deployment/kubernetes/viky-platform/custom-values/production.yaml --install "${NAMESPACE}-platform" --namespace "${NAMESPACE}" deployment/kubernetes/viky-platform/

# remove environment
delete_env:
  <<: *kubernetes_runner
  stage: admin
  except:
    - master
    - tags
  image: dtzar/helm-kubectl:2.13.1
  environment:
    name: ${CI_COMMIT_REF_NAME}
    url: https://viky-${CI_ENVIRONMENT_SLUG}-kube.viky.ai
    action: stop
  variables:
    NAMESPACE: viky-${CI_ENVIRONMENT_SLUG}
  script:
    - export KUBECONFIG=$KUBECONFIG
    - echo "Init helm ..."
    - helm init --client-only --wait
    - helm delete --purge "${NAMESPACE}-platform"
    - helm delete --purge "${NAMESPACE}-infra"
    - kubectl delete namespace ${NAMESPACE}
  when: manual
  allow_failure: true

# invite admin user
invite_admin_dev:
  <<: *kubernetes_runner
  stage: admin
  image: dtzar/helm-kubectl:2.13.1
  except:
    - master
    - tags
  environment:
    name: ${CI_COMMIT_REF_NAME}
    url: https://viky-${CI_ENVIRONMENT_SLUG}-kube.viky.ai
  variables:
    GIT_STRATEGY: none
    NAMESPACE: viky-${CI_ENVIRONMENT_SLUG}
  script:
    - export KUBECONFIG=$KUBECONFIG
    - echo "Init helm ..."
    - helm init --client-only --wait
    - PODNAME=$(kubectl get pods --namespace=$NAMESPACE -l app=webapp --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl --namespace=$NAMESPACE exec $PODNAME ./bin/rails users:invite_admin[${GITLAB_USER_EMAIL}]
  when: manual
  allow_failure: true

# invite admin user
invite_admin_preprod:
  <<: *kubernetes_runner
  stage: admin
  image: dtzar/helm-kubectl:2.13.1
  only:
    - master
  environment:
    name: viky-preprod
    url: https://viky-beta-kube.viky.ai
  variables:
    GIT_STRATEGY: none
    NAMESPACE: beta
  script:
    - export KUBECONFIG=$KUBECONFIG
    - PODNAME=$(kubectl get pods --namespace=$NAMESPACE -l app=webapp --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl --namespace=$NAMESPACE exec $PODNAME ./bin/rails users:invite_admin[${GITLAB_USER_EMAIL}]
  when: manual
  allow_failure: true

# backup
backup_dev:
  <<: *kubernetes_runner
  stage: admin
  image: dtzar/helm-kubectl:2.13.1
  dependencies: []
  except:
    - master
    - tags
  environment:
    name: ${CI_COMMIT_REF_NAME}
    url: https://viky-${CI_ENVIRONMENT_SLUG}-kube.viky.ai
  variables:
    GIT_STRATEGY: none
    NAMESPACE: viky-${CI_ENVIRONMENT_SLUG}
  script:
    - export KUBECONFIG=$KUBECONFIG
    - PODNAME=$(kubectl get pods --namespace=$NAMESPACE -l app=webapp-backup --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl exec --namespace=$NAMESPACE $PODNAME  bash /backup/backup.sh "${NAMESPACE}"
  when: manual
  allow_failure: true

# backup
backup_preprod:
  <<: *kubernetes_runner
  stage: admin
  image: dtzar/helm-kubectl:2.13.1
  dependencies: []
  only:
    - master
  environment:
    name: viky-preprod
    url: https://viky-beta-kube.viky.ai
  variables:
    GIT_STRATEGY: none
    NAMESPACE: beta
  script:
    - export KUBECONFIG=$KUBECONFIG
    - PODNAME=$(kubectl get pods --namespace=$NAMESPACE -l app=webapp-backup --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl exec --namespace=$NAMESPACE $PODNAME  bash /backup/backup.sh "${NAMESPACE}"
  when: manual
  allow_failure: true

# backup restore
backup_restore_dev:
  <<: *kubernetes_runner
  stage: admin
  image: dtzar/helm-kubectl:2.13.1
  dependencies: []
  except:
    - master
    - tags
  environment:
    name: ${CI_COMMIT_REF_NAME}
    url: https://viky-${CI_ENVIRONMENT_SLUG}-kube.viky.ai
  variables:
    GIT_STRATEGY: none
    NAMESPACE: viky-${CI_ENVIRONMENT_SLUG}
  script:
    - export KUBECONFIG=$KUBECONFIG
    - PODNAME_REDIS=$(kubectl get pods --namespace=$NAMESPACE -l app=redis --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl exec --namespace=$NAMESPACE $PODNAME_REDIS redis-cli flushall
    - PODNAME=$(kubectl get pods --namespace=$NAMESPACE -l app=webapp-backup --no-headers --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' | head -n1)
    - kubectl exec --namespace=$NAMESPACE $PODNAME  bash /backup/backup.sh "${NAMESPACE}"
    - kubectl exec --namespace=$NAMESPACE $PODNAME  bash /backup/restore.sh "${NAMESPACE}"
    - helm upgrade --force --wait --values custom-values.yml,deployment/kubernetes/viky-platform/custom-values/production.yaml --install "${NAMESPACE}-platform" --namespace "${NAMESPACE}" deployment/kubernetes/viky-platform/
  when: manual
  allow_failure: true

before_script:
  - |
    # Auto DevOps variables and functions
    [[ "${TRACE:-0}" == "1" ]] && set -x

    function viky_backup ()
    {
      echo "Backing up data ..."
      rancher_exec preprod ${VIKYAPP_DEPLOY_RANCHER_STACK}-backup-1 bash /backup/backup.sh "${NAMESPACE}"

      if [ $? -ne 0 ]; then
        (>&2 echo "Backing up data FAILED")
      fi
      echo ""
      return 0
    }

    function docker_build ()
    {
      # login to private registry
      docker_login

      local CI_DOCKER_IMAGE="$1"
      local CI_DOCKER_DIR="$2"
      local CI_DOCKER_IMAGE_BUILD_OPT="$3"

      echo "Build docker image ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} from dir ${CI_DOCKER_DIR} ..."
      docker build --pull ${CI_DOCKER_IMAGE_BUILD_OPT} -t ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} ${CI_DOCKER_DIR}
      echo ""

      echo "Taggging ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} to ${CI_DOCKER_IMAGE}:latest"
      docker tag ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} ${CI_DOCKER_IMAGE}:latest

      echo "Taggging ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} to ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}"
      docker tag ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}

      echo ""
      echo "Pushing ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} to GitLab Container Registry ..."
      docker push ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}

      echo ""
      echo "Pushing ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG} to GitLab Container Registry ..."
      docker push ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}

      echo ""
    }

    function docker_tag_latest ()
    {
      # login to private registry
      docker_login

      local CI_DOCKER_IMAGE="$1"

      echo "Pulling ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}"
      docker pull ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID}

      echo "Taggging ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} to ${CI_DOCKER_IMAGE}:latest"
      docker tag ${CI_DOCKER_IMAGE}:${CI_COMMIT_REF_SLUG}_${CI_PIPELINE_ID} ${CI_DOCKER_IMAGE}:latest

      echo "Pushing ${CI_DOCKER_IMAGE}:latest to GitLab Container Registry ..."
      docker push ${CI_DOCKER_IMAGE}:latest
      echo ""
    }

    function docker_login ()
    {
      if [[ -n "$CI_REGISTRY_USER" ]]; then
        echo "Logging to GitLab Container Registry with CI credentials ..."
        echo "${CI_REGISTRY_PASSWORD}" | docker login --username "${CI_REGISTRY_USER}" --password-stdin "${CI_REGISTRY}"
        echo ""
      fi
    }

    function rancher_exec ()
    {
      local RANCHER_ENV=$1

      # simulate a tty with script command https://github.com/rancher/rancher/issues/7971#issuecomment-286856228
      local CURRENT_COMMAND="rancher --wait --environment ${RANCHER_ENV} exec -t ${@:2}"
      echo "Exec : ${CURRENT_COMMAND}"
      script --return --flush --quiet --command "${CURRENT_COMMAND}" /dev/null
      return $?
    }
